{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3367c2",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21d08e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install fiftyone wandb open-clip-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6f506",
   "metadata": {},
   "source": [
    "## Part 1: Image Generation and Embedding Extraction\n",
    "In this section, you will load the pre-trained U-Net model from notebook 05_CLIP.ipynb, generate images of flowers, and extract embeddings from the model's bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils import UNet_utils, ddpm_utils\n",
    "\n",
    "# TODO: Initialize the U-Net model and load the pre-trained weights from notebook 05.\n",
    "\n",
    "# Make sure to use the same architecture as in the notebook.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet_utils.UNet(\n",
    "    T=400, img_ch=3, img_size=32, down_chs=(256, 256, 512), t_embed_dim=8, c_embed_dim=512\n",
    ").to(device)\n",
    "\n",
    "# TODO\n",
    "# model.load_state_dict(torch.load('path_to_your_model.pth')) # You need to provide the path to your trained model\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# TODO: Define a list of text prompts to generate images for.\n",
    "text_prompts = [\n",
    "\n",
    "    \"A photo of a red rose\",\n",
    "    \"A photo of a white daisy\",\n",
    "    \"A photo of a yellow sunflower\"\n",
    "\n",
    "]\n",
    "\n",
    "# --- Embedding Extraction using Hooks ---\n",
    "# We will use PyTorch hooks to extract the output of the 'down2' layer (the bottleneck).\n",
    "\n",
    "embeddings_storage = {}\n",
    "\n",
    "def get_embedding_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        embeddings_storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# TODO: Register a forward hook on the `down2` layer of the U-Net model.\n",
    "\n",
    "# The hook should store the output of the layer in the `embeddings_storage` dictionary.\n",
    "# model.down2.register_forward_hook(get_embedding_hook('down2'))\n",
    "\n",
    "# TODO: Modify the `sample_flowers` function from notebook 05 to generate images \n",
    "# and store the extracted embeddings.\n",
    "\n",
    "# You will need to run the generation process and then access the `embeddings_storage`\n",
    "# to get the embeddings for each generated image.\n",
    "# generated_images, _ = sample_flowers(text_prompts)\n",
    "# extracted_embeddings = embeddings_storage['down2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aefe8a",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation with CLIP Score and Frechet Inception Distance\n",
    "Now, evaluate the quality of your generated images using the measures described in the Metrics Calculation Guide section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d566d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "# TODO: Calculate the CLIP score for each generated image against its prompt.\n",
    "# You can use the `calculate_clip_score` function from the evaluation guide.\n",
    "\n",
    "# TODO: Calculate the FID score for the set of generated images.\n",
    "# You will need the `calculate_fid` function and the Inception model from the evaluation guide.\n",
    "# You will also need to load the real TF-Flowers dataset to compare against.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e54d2c",
   "metadata": {},
   "source": [
    "## Part 3: Embedding Analysis with FiftyOne Brain\n",
    "In this section, you will use FiftyOne to analyze the embeddings you extracted from the U-Net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "# TODO: Create a new FiftyOne dataset.\n",
    "\n",
    "dataset = fo.Dataset(name=\"generated_flowers_with_embeddings\")\n",
    "\n",
    "# TODO: Iterate through your generated images and add them to the dataset.\n",
    "# For each image, create a fiftyone.Sample and add the following metadata:\n",
    "# - The file path to the saved image.\n",
    "# - The text prompt (as a `fo.Classification` label).\n",
    "# - The CLIP score (as a custom field).\n",
    "# - The extracted U-Net embedding (as a custom field).\n",
    "\n",
    "# TODO: Compute uniqueness for the dataset.\n",
    "# fob.compute_uniqueness(dataset)\n",
    "\n",
    "# TODO: Compute representativeness using the extracted U-Net embeddings.\n",
    "# fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")\n",
    "\n",
    "# TODO: Launch the FiftyOne App to visualize your dataset and analyze the results.\n",
    "# session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00839fd2",
   "metadata": {},
   "source": [
    "## Part 4: Logging with Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# TODO: Login to wandb.\n",
    "# wandb.login()\n",
    "\n",
    "# TODO: Initialize a new wandb run.\n",
    "# run = wandb.init(project=\"diffusion_model_assessment_v2\")\n",
    "\n",
    "# TODO: Log your hyperparameters (e.g., guidance weight `w`, number of steps `T`).\n",
    "\n",
    "# TODO: Log your evaluation metrics (CLIP Score and FID).\n",
    "\n",
    "# TODO: Create a wandb.Table to log your results. The table should include:\n",
    "# - The generated image.\n",
    "# - The text prompt.\n",
    "# - The CLIP score.\n",
    "# - The uniqueness score.\n",
    "# - The representativeness score.\n",
    "\n",
    "# TODO: Finish the wandb run.\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec59241",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed2670",
   "metadata": {},
   "source": [
    "CLIP Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e62e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import open_clip\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_clip_score(image_path, text_prompt):\n",
    "\n",
    "    # Load model\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "\n",
    "    # Preprocess inputs\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    text = tokenizer([text_prompt])\n",
    "\n",
    "    # Compute features and similarity\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "        # Normalize features\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Calculate dot product\n",
    "        score = (image_features @ text_features.T).item()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3769d7",
   "metadata": {},
   "source": [
    "Fr√©chet Inception Distance (FID) FID measures the distance between the feature distributions of real images and generated images. Lower scores indicate that the generated images possess visual quality and diversity similar to the real dataset. Note that this metric is defined through the InceptionV3 model and you have to use an ImageNet pre-trained InceptionV3 model to compute it. Here's a demo notebook to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922da1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_fid(real_embeddings, gen_embeddings):\n",
    "\n",
    "    # real_embeddings and gen_embeddings should be Numpy arrays of shape (N, 2048) \n",
    "    # extracted from an InceptionV3 model\n",
    "\n",
    "    # Calculate mean and covariance\n",
    "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
    "    mu2, sigma2 = gen_embeddings.mean(axis=0), np.cov(gen_embeddings, rowvar=False)\n",
    "\n",
    "    # Calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "\n",
    "    # Calculate sqrt of product of covariances\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # Handle numerical errors\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # Final FID calculation\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
